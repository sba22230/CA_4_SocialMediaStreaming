{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cef3a63",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis for the word Euro\n",
    "\n",
    "Connect Spark to Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df04beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars \"/usr/local/spark/jars/mongo-spark-connector_2.12-3.0.2.jar,/usr/local/spark/jars/mongo-java-driver-3.12.9.jar\" pyspark-shell'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c12edd37",
   "metadata": {},
   "source": [
    "Note the inferschema is set to false, this makes spark read the entire database and not infer the values of fields from the first set of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84472e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# fix read bug, basically turn off sampling\n",
    "spark = SparkSession.builder.appName(\"TwitterMongo\") \\\n",
    ".config(\"spark.mongodb.input.database\", \"mongodb://localhost:27017/twitter\") \\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/twitter.tweets\") \\\n",
    ".config(\"spark.mongodb.read.sql.inferSchema.mapTypes.enabled\", \"FALSE\") \\\n",
    ".config(\"spark.mongodb.output.uri\",\"mongodb://localhost:27017/twitter.tweets\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4badca7e",
   "metadata": {},
   "source": [
    "## Create the Session\n",
    "\n",
    "And load all of the Twitter data in MongoDB\n",
    "\n",
    "Print out the twitter tweet schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"ABC\") \\\n",
    ".config(\"spark.driver.memory\", \"15g\") \\\n",
    ".config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/twitter\") \\\n",
    ".config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/twitter\") \\\n",
    ".config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:2.12-3.0.2') \\\n",
    ".getOrCreate()\n",
    "# read data from mongodb collection \"questions\" into a dataframe \"df\"\n",
    "df = spark.read \\\n",
    ".format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    ".option(\"uri\", \"mongodb://localhost:27017/twitter\") \\\n",
    ".option(\"database\", \"twitter\") \\\n",
    ".option(\"collection\", \"tweets\") \\\n",
    ".load()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b68eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "404e0b94",
   "metadata": {},
   "source": [
    "Create a spark object of the tweets held in the mongo db \n",
    "\n",
    "It is easier to use SQL statements and Pyspark to clean the data rather than writing queries in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330eb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ffce3ac",
   "metadata": {},
   "source": [
    "How many tweets in the DB all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT DISTINCT id FROM tweets\")\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55a62139",
   "metadata": {},
   "source": [
    "How many tweets by language \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9015ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install plotly\n",
    "import pyspark.pandas as ps\n",
    "import plotly\n",
    "dfLang = spark.sql(\"SELECT DISTINCT lang, CAST(count(id) AS INT) as TweetCount FROM tweets GROUP BY lang \\\n",
    "                   ORDER BY TweetCount DESC LIMIT 10\")\n",
    "dfLang.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = ps.DataFrame(dfLang)\n",
    "\n",
    "tempdf.plot(kind='bar', x='lang', y='TweetCount')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bc9f4f8",
   "metadata": {},
   "source": [
    "How many tweets by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoc = spark.sql(\"SELECT DISTINCT user.location AS Location, CAST(count(id) AS INT) as TweetCount FROM tweets GROUP BY user.location \\\n",
    "                   ORDER BY TweetCount DESC LIMIT 10\")\n",
    "dfLoc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b94c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = ps.DataFrame(dfLoc)\n",
    "\n",
    "tempdf.plot(kind='bar', x='Location', y='TweetCount')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "106f0306",
   "metadata": {},
   "source": [
    "Now limit the dataset to English texts and tweets with the Euro in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41824ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEnTwt = spark.sql(\"SELECT * FROM tweets WHERE lang = 'en' AND text LIKE '%euro%'\")\n",
    "dfEnTwt.createOrReplaceTempView(\"en_tweets\")\n",
    "dfEnTwt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5654e5c3",
   "metadata": {},
   "source": [
    "Now how many tweets in the English language dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37eb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEnTwt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c186b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select tweet id, geo, lang, quoted_status,quoted_status.geo \n",
    "# Having a look at some the data\n",
    "dfOne = spark.sql(\"SELECT DISTINCT id,  text, quote_count, reply_count, retweet_count, favorite_count, geo, place, lang, \\\n",
    "                  quoted_status,quoted_status.geo, quoted_status.text, user.name, user.location   \\\n",
    "                  FROM en_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf79faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfDay= spark.sql(\"SELECT DISTINCT CAST(substring(created_at, 27, 4) AS INT) as Year, \\\n",
    "          CAST(from_unixtime(unix_timestamp(substring(created_at, 5, 3), 'MMM'), 'MM') As INT) as Month, \\\n",
    "          CAST(substring(created_at, 9, 2) AS INT) as Day, \\\n",
    "          CAST(count(id) AS INT) as TweetCount \\\n",
    "          FROM en_tweets \\\n",
    "          WHERE lang = 'en' AND text LIKE '%euro%' GROUP BY substring(created_at, 27, 4), \\\n",
    "          substring(created_at, 5, 3), \\\n",
    "          substring(created_at, 9, 2)\")\n",
    "dfDay.createOrReplaceTempView(\"tweetsByDay\")\n",
    "\n",
    "dfDay = spark.sql(\"SELECT * FROM tweetsByDay ORDER BY Year, Month, Day\")\n",
    "\n",
    "dfDay.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccccb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = ps.DataFrame(dfDay)\n",
    "\n",
    "tempdf.plot(kind='bar', x='Day', y='TweetCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeek = spark.sql(\"SELECT Year, weekofyear(make_date(Year, Month, Day)) as wkofYr , SUM(TweetCount) as TweetCountbyWeek \\\n",
    "                   FROM tweetsByDay \\\n",
    "                   GROUP BY Year, weekofyear(make_date(Year, Month, Day))\")\n",
    "\n",
    "dfWeek.createOrReplaceTempView(\"tweetsByWeek\")\n",
    "dfWeek.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a883c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = ps.DataFrame(dfWeek)\n",
    "\n",
    "tempdf.plot(kind='bar', x='wkofYr', y='TweetCountbyWeek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tweet count by month\n",
    "dfMonth = spark.sql(\"SELECT Year, Month, SUM(TweetCount) as TweetCountbyMonth \\\n",
    "                     FROM tweetsByDay GROUP BY Year, Month\")\n",
    "dfMonth.createOrReplaceTempView(\"tweetsByMonth\")\n",
    "dfMonth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = ps.DataFrame(dfMonth)\n",
    "\n",
    "tempdf.plot(kind='bar', x='Month', y='TweetCountbyMonth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c854ddd",
   "metadata": {},
   "source": [
    "### Next up is text clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install wordcloud\n",
    "#%pip install vadersentiment\n",
    "## sentiment analysis ref: https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/?ref=lbp\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    " \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "     \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    #print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    #print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    #print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    #print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "       print(\"Negative\")\n",
    " \n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "    \n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText = spark.sql(\"SELECT DISTINCT LOWER(text) AS text FROM en_tweets\")\n",
    "dfText.createOrReplaceTempView(\"text\")\n",
    "dfText.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e65d385",
   "metadata": {},
   "source": [
    "Count the number of words in the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "dfWord = spark.sql(\"SELECT explode(split(text, ' ')) AS word FROM text\")\n",
    "\n",
    "dfWord.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a213b15",
   "metadata": {},
   "source": [
    "Count the number of characters including spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfChar = spark.sql(\"SELECT len(text) AS char FROM text\")\n",
    "dfChar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f71dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "dfText = spark.sql(\"SELECT * FROM text\")\n",
    "dfText = dfText.withColumn(\"text\", regexp_replace('text', pattern, ''))\n",
    "dfText.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.createOrReplaceTempView(\"text\") # this is cleaned txt\n",
    "dfText = spark.sql(\"SELECT TRIM(text) AS text FROM text\")\n",
    "\n",
    "dfText.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5673de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now build a word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "dfText.createOrReplaceTempView(\"text\") # this is the trimmed txt\n",
    "dfText = spark.sql(\"SELECT text FROM text\")\n",
    "dfText.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cfa2907",
   "metadata": {},
   "source": [
    "### Tokenize and Stem the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stackoverflow ref: https://stackoverflow.com/questions/53579444\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "dfText = tokenizer.transform(dfText).select(\"text\",\"tokens\")\n",
    "\n",
    "dfText.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "dfText = remover.transform(dfText).select(\"text\",\"filtered\")\n",
    "\n",
    "dfText.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "dfText = dfText.withColumn(\"filtered_stemmed\", stemmer_udf(\"filtered\"))\n",
    "dfText.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out short words\n",
    "filterShortWords = udf(lambda row: [x for x in row if len(x) >= 4], ArrayType(StringType()))\n",
    "dfText = dfText.withColumn(\"filtered_stemmed\", filterShortWords(\"filtered_stemmed\"))\n",
    "\n",
    "dfText.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae79004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
